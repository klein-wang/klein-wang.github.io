<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.2">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.3/css/all.min.css" integrity="sha256-2H3fkXt6FEmrReK448mDVGKb3WW2ZZw35gI7vqHOE4Y=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Pisces","version":"8.5.0","exturl":false,"sidebar":{"position":"right","width":240,"display":"always","padding":18,"offset":12},"copycode":false,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The model achieves 28.4 BLEU on the WMT 2014">
<meta property="og:type" content="article">
<meta property="og:title" content="Attention Is All You Need">
<meta property="og:url" content="http://example.com/2021/08/17/p7/index.html">
<meta property="og:site_name" content="My Channel">
<meta property="og:description" content="We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The model achieves 28.4 BLEU on the WMT 2014">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_t+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-3ebc69bc8606497dc9ee9e2f89b99c3d_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=Attention(Q,K,V)=softmax(%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D)V">
<meta property="og:image" content="https://pic4.zhimg.com/80/v2-fb47315cfee387e13803197ae64ab19b_1440w.jpg">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=MultiHead(Q,K,V)=Concat(head_1,+...,+head_h)W%5EO,+">
<meta property="og:image" content="https://www.zhihu.com/equation?tex=%5Cmbox%7Bwhere+%7D+head_i=Attention(QW%5EQ_i,KW%5EK_i,VW%5EV_i)">
<meta property="og:image" content="https://pic2.zhimg.com/80/v2-9d0bc477be197df621aca2ed01a69e5d_1440w.jpg">
<meta property="og:image" content="https://pic3.zhimg.com/80/v2-5291c76e7a50ba517ecba226297e4ebe_1440w.jpg">
<meta property="article:published_time" content="2021-08-17T08:40:20.000Z">
<meta property="article:modified_time" content="2021-08-20T03:47:34.000Z">
<meta property="article:author" content="Klein">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="NLP">
<meta property="article:tag" content="Self-Attention">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.zhihu.com/equation?tex=h_t+">


<link rel="canonical" href="http://example.com/2021/08/17/p7/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2021/08/17/p7/","path":"2021/08/17/p7/","title":"Attention Is All You Need"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Attention Is All You Need | My Channel</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">My Channel</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">1.1</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li>
        <li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags<span class="badge">33</span></a></li>
        <li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories<span class="badge">5</span></a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives<span class="badge">19</span></a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Abstract"><span class="nav-number">1.</span> <span class="nav-text">Abstract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Limitation-of-RNN-model"><span class="nav-number">2.</span> <span class="nav-text">Limitation of RNN model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Why-Self-Attention"><span class="nav-number">3.</span> <span class="nav-text">Why Self-Attention</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Model-Architecture-Transformer"><span class="nav-number">4.</span> <span class="nav-text">Model Architecture (Transformer)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conclusion"><span class="nav-number">5.</span> <span class="nav-text">Conclusion</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-overview">
            <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Klein"
      src="/images/avatar.gif">
  <p class="site-author-name" itemprop="name">Klein</p>
  <div class="site-description" itemprop="description">Personal website for idea spreading.</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">19</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">33</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author site-overview-item animated">
      <span class="links-of-author-item">
        <a href="https://github.com/klein-wang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;klein-wang" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:kleinwang1@gmail.com" title="G-Mail → mailto:kleinwang1@gmail.com" rel="noopener" target="_blank"><i class="fab fa-google fa-fw"></i>G-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:2370347112@qq.com" title="QQ-Mail → mailto:2370347112@qq.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>QQ-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.facebook.com/profile.php?id=100011302230273" title="FB Page → https:&#x2F;&#x2F;www.facebook.com&#x2F;profile.php?id&#x3D;100011302230273" rel="noopener" target="_blank"><i class="fab fa-facebook fa-fw"></i>FB Page</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.youtube.com/channel/UCcT2r9gIYAcPBw6q0qY685Q/playlists" title="YouTube → https:&#x2F;&#x2F;www.youtube.com&#x2F;channel&#x2F;UCcT2r9gIYAcPBw6q0qY685Q&#x2F;playlists" rel="noopener" target="_blank"><i class="fab fa-youtube fa-fw"></i>YouTube</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.instagram.com/kleinwang" title="Instagram → https:&#x2F;&#x2F;www.instagram.com&#x2F;kleinwang" rel="noopener" target="_blank"><i class="fab fa-instagram fa-fw"></i>Instagram</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/yuanchen-klein-wang-87004a112/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;yuanchen-klein-wang-87004a112&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/wang-yuan-chen-24/columns" title="Zhihu → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;wang-yuan-chen-24&#x2F;columns" rel="noopener" target="_blank"><i class="fab fa-zhihu fa-fw"></i>Zhihu</a>
      </span>
  </div>
  <div class="cc-license site-overview-item animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdn.jsdelivr.net/npm/@creativecommons/vocabulary@2020.11.3/assets/license_badges/small/by_nc_sa.svg" alt="Creative Commons"></a>
  </div>



          </div>
        </div>
        <div class="back-to-top animated" role="button" aria-label="Back to top">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/08/17/p7/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Klein">
      <meta itemprop="description" content="Personal website for idea spreading.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="My Channel">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Attention Is All You Need
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2021-08-17 16:40:20" itemprop="dateCreated datePublished" datetime="2021-08-17T16:40:20+08:00">2021-08-17</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">Edited on</span>
        <time title="Modified: 2021-08-20 11:47:34" itemprop="dateModified" datetime="2021-08-20T11:47:34+08:00">2021-08-20</time>
      </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/academics/" itemprop="url" rel="index"><span itemprop="name">academics</span></a>
        </span>
          , 
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/academics/journal-review/" itemprop="url" rel="index"><span itemprop="name">journal review</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

            <div class="post-description">We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p><strong>Attention Is All You Need</strong></p>
<p>Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin</p>
<p>Link: <a href="https://link.zhihu.com/?target=https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><blockquote>
<p>The dominant sequence transduction models are based on complex recurrent or convolutional neural networks (RNN, CNN) in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. </p>
<p>We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.</p>
</blockquote>
<h2 id="Limitation-of-RNN-model"><a href="#Limitation-of-RNN-model" class="headerlink" title="Limitation of RNN model"></a>Limitation of RNN model</h2><p>Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states <img src="https://www.zhihu.com/equation?tex=h_t+"> , as a function of the previous hidden state <img src="https://www.zhihu.com/equation?tex=h_%7Bt-1%7D"> and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. </p>
<h2 id="Why-Self-Attention"><a href="#Why-Self-Attention" class="headerlink" title="Why Self-Attention"></a>Why Self-Attention</h2><p>First advantage is the total computational complexity per layer. When it comes to self-attention, the amount of computation can be parallelized, as measured by the minimum number of sequential operations required. </p>
<p><img src="https://pic2.zhimg.com/80/v2-3ebc69bc8606497dc9ee9e2f89b99c3d_1440w.jpg"></p>
<p>Also, as a self-attention layer connects all positions with a constant number of sequentially executed operations, the overall model improves the difficulties in reducing path length in long-range dependencies in the network.</p>
<h2 id="Model-Architecture-Transformer"><a href="#Model-Architecture-Transformer" class="headerlink" title="Model Architecture (Transformer)"></a>Model Architecture (Transformer)</h2><p>Instead of paying attention to the last state of the encoder as is usually done with RNNs, in each step of the decoder we look at all the states of the encoder, being able to access information about all the elements of the input sequence. This is what attention does, it extracts information from the whole sequence, a weighted sum of all the past encoder states. This allows the decoder to assign greater weight or importance to a certain element of the input for each element of the output.</p>
<p>Within the self-attention mechanism of the model, there are 3 elements being introduced in the paper: <strong>The Query, The Value and The Key</strong>. The model is computing the dot product of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.</p>
<p><img src="https://www.zhihu.com/equation?tex=Attention(Q,K,V)=softmax(%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D)V"></p>
<p>The attention scores measure how much focus to place on other places or words of the input sequence w.r.t a word at a certain position. That is, the dot product of the query vector with the key vector of the respective word we’re scoring. </p>
<p><strong>Multi-head Attention</strong></p>
<p>In the previous description the attention scores are focused on the whole sentence at a time, this would produce the same results even if two sentences contain the same words in a different order. Instead, authors in the paper proposed Multi-head Attention, which they added attention to different segments of the words. </p>
<blockquote>
<p>“We can give the self-attention greater power of discrimination, by combining several self-attention heads, dividing the words vectors into a fixed number (h, number of heads) of chunks, and then self-attention is applied on the corresponding chunks, using Q, K and V sub-matrices.”, Peter Bloem. </p>
</blockquote>
<p><img src="https://pic4.zhimg.com/80/v2-fb47315cfee387e13803197ae64ab19b_1440w.jpg"></p>
<p>After calculating the dot product of every head, we concatenate the output matrices and multiply them by an additional weights matrix. This final matrix captures information from all the attention heads.</p>
<p><img src="https://www.zhihu.com/equation?tex=MultiHead(Q,K,V)=Concat(head_1,+...,+head_h)W%5EO,+"></p>
<p><img src="https://www.zhihu.com/equation?tex=%5Cmbox%7Bwhere+%7D+head_i=Attention(QW%5EQ_i,KW%5EK_i,VW%5EV_i)"></p>
<p>The paper also addressed positional encoding approach to solve the issue with the order of the words in the sentence. Briefly speaking, the same scores would be computed even if we shuffle up the words in an input sentence, due to permutation invariant characteristic of the self-attention mechanism. </p>
<p>The authors has applied a function to map the position in the sentence to a real valued vector. The network will then learn how to use this information. Another approach would be to use a position embedding, similar to word embedding, coding every known position with a vector.</p>
<p>Next, a residual connection is introduced around each sub-layer (attention and Fully connected fee-forward network, summing up the output of the layer with its input, followed by a layer normalization. Before every residual connection, a regularization is applied: </p>
<p>“We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks” with a dropout rate of 0.1.</p>
<p>The figure below shows the components detailed:</p>
<p><img src="https://pic2.zhimg.com/80/v2-9d0bc477be197df621aca2ed01a69e5d_1440w.jpg"></p>
<p>There are many other details which I haven’t touched on here. Feel free to discuss more thoughts in this open space . The paper offers an excellent opportunity to people who are interested in technical innovation on sequence-sequence translation tasks. The article in <strong>towardsdatascience</strong> also did a great job in explaining the transformer model regarding this paper. You can read it from <a target="_blank" rel="noopener" href="https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634">here</a>. </p>
<h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>The model performance was evaluated by looking at the BLEU score on the English-to-German and English-to-French newstest2014 tests as a fraction of the training cost. </p>
<p><img src="https://pic3.zhimg.com/80/v2-5291c76e7a50ba517ecba226297e4ebe_1440w.jpg"></p>
<p>The table here summarizes the results and compares models’ translation quality and training costs to other model architectures from the literature. </p>
<p>For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers, specifically on both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.</p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>Buy me a coffee</div>
  <button onclick="document.querySelector('.post-reward').classList.toggle('active');">
    Donate
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.png" alt="Klein WeChat Pay">
        <span>WeChat Pay</span>
      </div>
      <div>
        <img src="/images/alipay.png" alt="Klein Alipay">
        <span>Alipay</span>
      </div>

  </div>
</div>

          <div class="followme">
  <span>Welcome to my other publishing channels</span>

  <div class="social-list">

      <div class="social-item">
        <a target="_blank" class="social-link" href="https://www.zhihu.com/people/wang-yuan-chen-24">
          <span class="icon">
            <i class="fab fa-zhihu"></i>
          </span>

          <span class="label">Zhihu</span>
        </a>
      </div>
  </div>
</div>

          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/NLP/" rel="tag"># NLP</a>
              <a href="/tags/Self-Attention/" rel="tag"># Self-Attention</a>
              <a href="/tags/Transformer/" rel="tag"># Transformer</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021/08/14/%E8%A5%BF%E7%BA%BF%E6%97%A0%E6%88%98%E4%BA%8B/" rel="prev" title="读书笔记《西线无战事》">
                  <i class="fa fa-chevron-left"></i> 读书笔记《西线无战事》
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2021/10/10/notes1/" rel="next" title="LSTM - Time Series Anomaly Detection">
                  LSTM - Time Series Anomaly Detection <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 2021 – 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Klein</span>
</div>
<div class="busuanzi-count">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>


<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("06/25/2021 12:00:00");//此处修改你的建站时间或者网站上线时间 
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "本站已安全运行 "+dnum+" 天 "; 
        document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒"; 
    } 
setInterval("createtime()",250);
</script>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.5.7/dist/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  
<script src="/js/third-party/search/local-search.js"></script>




  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>





<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":true}});</script></body>
</html>
